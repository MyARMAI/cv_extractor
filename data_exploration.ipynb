{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size : 24\n"
     ]
    }
   ],
   "source": [
    "import textract\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "#in case punkt package is missing decomment the line below\n",
    "#nltk.download(\"punkt\")\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "\n",
    "datadir = r\"C:\\Users\\Cheikh\\Desktop\\Projet_memoire\\myArmAi\\samples\\cv\\cv_atos\\fr\\word\"\n",
    "\n",
    "def loadData(path):\n",
    "    \n",
    "    if not os.path.isdir(path):\n",
    "            raise Exception(\"OpenFolderException\",\"The given path is not a valid folder or folder doesn't exist\")\n",
    "    _dataset = []\n",
    "    raw_dataset = [textract.process(os.path.join(datadir,f)).decode() for f in os.listdir(datadir)]\n",
    "    \n",
    "    print(\"Dataset size : {}\".format(len(raw_dataset)))\n",
    "    \n",
    "    for d in raw_dataset:\n",
    "        _dataset.append(d)\n",
    "    return _dataset;\n",
    "\n",
    "\n",
    "def dataSetInfo(data):\n",
    "    word_count = 0;\n",
    "    for _d in data:\n",
    "        word_count += len(_d)\n",
    "    return word_count;\n",
    "\n",
    "dataset = loadData(datadir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Abdoulaye BARRO                                                                                                                                             Ingénieur Etude et développement                                                                                                                           Formation     2018, Master en Big Data (AIMS de Mbour)                                     2017, Master en Science de l'ingénieur (option informatique)                 ESP                                                                          2015, Master en statistique appliquée (Université Gaston                     Berger) Mention ASSEZ BIEN                                                   2014, Maitrise en Mathématiques Appliquées et Informatique                   (Université Gaston Berger) Mention ASSEZ BIEN                                2010, Baccalauréat Scientifique. Mention PASSABLE              Langues       Anglais (Intermédiaire)                                                      Français (courant)                                             Localisation  Dakar (Keur Massar)                                            Principales compétences  Conception, Analyse, Développement                 Compétences              Analyse de cahier des charges                      fonctionnelles           Modélisation des données : modèle                                           entité-association                                                          Modélisation objet : UML                                                    Développement d'application                                                 Algorithme                                                                  Data Mining                                        Compétences techniques                                                                                   Domaine        Niveau   Domaine        Niveau   Langage/ Framework / BDD   SQL / SQL      2        JavaScript      2                                 Server                  /JQuery                                           Postgresql              Python                                            C/C++          1.5      Intégration Html2                                                         CSS                                               Java / Java JEE2        PHP / Bootstrap 2                                 Symfony        1.5      Django          1      Systèmes d'exploitation    LINUX          2        Windows         2.5    Outils                     Maven          1        UiPath          2.5                               Tomcat         2                        2                                 JBOSS (wildfly)1.5      RPA Express                                                               zeplin                 Modélisation               UML / BPMN     2                               Mathématiques                                                                 Atos                                                                        Déc 2018 à aujourd'hui     Covoiturage                                      Fonction                   Développeur Backend                              Projet                     Développement d'une application de covoiturage.  Mission(s) et              Analyse des besoins et conception                réalisations               Développement de l'application                                              Déploiement                                      Environnement              Java/JEE, jhipster, angular, HTML, CSS maven,                               git, intelli-J, Ionic, MySQL Workbench, JDL                                 Studio                                           Atos                                                                        Fev. 2017 à Déc 2018       Système de recommandation                        Fonction                   Data Scientist                                   Projet                     Développement d'un système de recommandation de                             profil                                           Mission(s) et              Analyse des besoins et conception                réalisations               Développement de la partie backend(python)                                  Développement de la partie front (Angular)       Environnement              Python, Flask, Angular, HTML, CSS, Material                                 design, Visual studio code                       Dimbl Holding service                                                       Fev. 2017 à juillet     Développement d'algorithme                          2017                                                                        Fonction                Stagiaire Data Scientist                            Projet                  Développement d'algorithme de Scraping (Rebot de                            collecte de données dans les sites web)             Mission(s) et           Développer des algorithmes de collecte de données   réalisations            Comparaison d'un programme écrit en python et celui                         écrit en C.                                         Environnement           R, Python, C                                                                                                                                                                                                    Certifications                                                                Microsoft: Programming in HTML5 with JavaScript and CSS3 (Examen 70-480)        Oracle Certified Associate Java Programmer OCA (En cour)                                                                                                                                                                                                                                                                        Centre d'intérêt                                                                                                                                                                                                                                JOGGING                                                                         Football                                                                        Lecture                                                                                                                                                         \""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter \n",
    "\n",
    "\n",
    "\n",
    "train,test =  dataset[0:11],dataset[11:]\n",
    "\n",
    "#2 tokenize : remove all no alphanumeric character and remove stopwords\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "def dataCleaning(raw_data):\n",
    "    cleaned_data = []\n",
    "    stop_words = set(stopwords.words(\"french\"))\n",
    "    for data in raw_data:\n",
    "        cleaned_data.append([x.lower() for x in tokenizer.tokenize(data) if x not in stop_words])\n",
    "    return cleaned_data;\n",
    "\n",
    "#train = dataCleaning(train)\n",
    "\n",
    "occurences  = Counter(train[0])\n",
    "\n",
    "train[0]\n",
    "train_ = re.sub(\"[\\\\r\\\\n\\|]*\",\"\",train[0])\n",
    "train_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wordDoc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-3406c844e7be>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtables\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtableToDF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwordDoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'wordDoc' is not defined"
     ]
    }
   ],
   "source": [
    "from docx import Document\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "#transform word table to pandas dataframe\n",
    "def tableToDF(wordDoc):\n",
    "    tables = []\n",
    "    for table in wordDoc.tables:\n",
    "        df = [['' for i in range(len(table.columns))] for j in range(len(table.rows))]\n",
    "        for i, row in enumerate(table.rows):\n",
    "            for j, cell in enumerate(row.cells):\n",
    "                if cell.text:\n",
    "                    df[i][j] = cell.text\n",
    "        tables.append(pd.DataFrame(df))\n",
    "    return tables;\n",
    "\n",
    "res = tableToDF(wordDoc)\n",
    "print(len(res))\n",
    "res[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = u\"./resume_sentence_dataset.json\"\n",
    "\n",
    "\n",
    "data = pd.read_csv(filepath)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
