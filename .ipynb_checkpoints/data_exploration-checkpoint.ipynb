{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size : 24\n"
     ]
    }
   ],
   "source": [
    "import textract\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "#in case punkt package is missing decomment the line below\n",
    "#nltk.download(\"punkt\")\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "\n",
    "datadir = r\"C:\\Users\\Cheikh\\Desktop\\Projet_memoire\\myArmAi\\samples\\cv\\cv_atos\\fr\\word\"\n",
    "\n",
    "def loadData(path):\n",
    "    \n",
    "    if not os.path.isdir(path):\n",
    "            raise Exception(\"OpenFolderException\",\"The given path is not a valid folder or folder doesn't exist\")\n",
    "    _dataset = []\n",
    "    raw_dataset = [textract.process(os.path.join(datadir,f)).decode() for f in os.listdir(datadir)]\n",
    "    \n",
    "    print(\"Dataset size : {}\".format(len(raw_dataset)))\n",
    "    \n",
    "    for d in raw_dataset:\n",
    "        _dataset.append(d)\n",
    "    return _dataset;\n",
    "\n",
    "\n",
    "def dataSetInfo(data):\n",
    "    word_count = 0;\n",
    "    for _d in data:\n",
    "        word_count += len(_d)\n",
    "    return word_count;\n",
    "\n",
    "dataset = loadData(datadir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "|Abdoulaye BARRO                                    |                           |\r\n",
      "|                                                   |                           |\r\n",
      "|Ingénieur Etude et développement                   |                           |\r\n",
      "\r\n",
      "|             | |                                                               |\r\n",
      "|Formation    | |2018, Master en Big Data (AIMS de Mbour)                       |\r\n",
      "|             | |2017, Master en Science de l'ingénieur (option informatique)   |\r\n",
      "|             | |ESP                                                            |\r\n",
      "|             | |2015, Master en statistique appliquée (Université Gaston       |\r\n",
      "|             | |Berger) Mention ASSEZ BIEN                                     |\r\n",
      "|             | |2014, Maitrise en Mathématiques Appliquées et Informatique     |\r\n",
      "|             | |(Université Gaston Berger) Mention ASSEZ BIEN                  |\r\n",
      "|             | |2010, Baccalauréat Scientifique. Mention PASSABLE              |\r\n",
      "|Langues      | |Anglais (Intermédiaire)                                        |\r\n",
      "|             | |Français (courant)                                             |\r\n",
      "|Localisation | |Dakar (Keur Massar)                                            |\r\n",
      "\r\n",
      "\r\n",
      "|Principales compétences | |Conception, Analyse, Développement                 |\r\n",
      "|Compétences             | |Analyse de cahier des charges                      |\r\n",
      "|fonctionnelles          | |Modélisation des données : modèle                  |\r\n",
      "|                        | |entité-association                                 |\r\n",
      "|                        | |Modélisation objet : UML                           |\r\n",
      "|                        | |Développement d'application                        |\r\n",
      "|                        | |Algorithme                                         |\r\n",
      "|                        | |Data Mining                                        |\r\n",
      "\r\n",
      "\r\n",
      "|Compétences techniques                                                        |\r\n",
      "\r\n",
      "|                           |Domaine        |Niveau  | |Domaine        |Niveau   |\r\n",
      "|Langage/ Framework / BDD   |SQL / SQL      |2       | |JavaScript      |2      |\r\n",
      "|                           |Server         |        | |/JQuery         |       |\r\n",
      "|                           |Postgresql     |        | |Python          |       |\r\n",
      "|                           |C/C++          |1.5     | |Intégration Html|2      |\r\n",
      "|                           |               |        | |CSS             |       |\r\n",
      "|                           |Java / Java JEE|2       | |PHP / Bootstrap |2      |\r\n",
      "|                           |Symfony        |1.5     | |Django          |1      |\r\n",
      "|Systèmes d'exploitation    |LINUX          |2       | |Windows         |2.5    |\r\n",
      "|Outils                     |Maven          |1       | |UiPath          |2.5    |\r\n",
      "|                           |Tomcat         |2       | |                |2      |\r\n",
      "|                           |JBOSS (wildfly)|1.5     | |RPA Express     |       |\r\n",
      "|                           |               |        | |zeplin          |       |\r\n",
      "|Modélisation               |UML / BPMN     |2       | |                |       |\r\n",
      "|Mathématiques                                                                 |\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "|Atos                  |     |                                                 |\r\n",
      "|Déc 2018 à aujourd'hui|     |Covoiturage                                      |\r\n",
      "|Fonction              |     |Développeur Backend                              |\r\n",
      "|Projet                |     |Développement d'une application de covoiturage.  |\r\n",
      "|Mission(s) et         |     |Analyse des besoins et conception                |\r\n",
      "|réalisations          |     |Développement de l'application                   |\r\n",
      "|                      |     |Déploiement                                      |\r\n",
      "|Environnement         |     |Java/JEE, jhipster, angular, HTML, CSS maven,    |\r\n",
      "|                      |     |git, intelli-J, Ionic, MySQL Workbench, JDL      |\r\n",
      "|                      |     |Studio                                           |\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "|Atos                  |     |                                                 |\r\n",
      "|Fev. 2017 à Déc 2018  |     |Système de recommandation                        |\r\n",
      "|Fonction              |     |Data Scientist                                   |\r\n",
      "|Projet                |     |Développement d'un système de recommandation de  |\r\n",
      "|                      |     |profil                                           |\r\n",
      "|Mission(s) et         |     |Analyse des besoins et conception                |\r\n",
      "|réalisations          |     |Développement de la partie backend(python)       |\r\n",
      "|                      |     |Développement de la partie front (Angular)       |\r\n",
      "|Environnement         |     |Python, Flask, Angular, HTML, CSS, Material      |\r\n",
      "|                      |     |design, Visual studio code                       |\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "|Dimbl Holding service |  |                                                    |\r\n",
      "|Fev. 2017 à juillet   |  |Développement d'algorithme                          |\r\n",
      "|2017                  |  |                                                    |\r\n",
      "|Fonction              |  |Stagiaire Data Scientist                            |\r\n",
      "|Projet                |  |Développement d'algorithme de Scraping (Rebot de    |\r\n",
      "|                      |  |collecte de données dans les sites web)             |\r\n",
      "|Mission(s) et         |  |Développer des algorithmes de collecte de données   |\r\n",
      "|réalisations          |  |Comparaison d'un programme écrit en python et celui |\r\n",
      "|                      |  |écrit en C.                                         |\r\n",
      "|Environnement         |  |R, Python, C                                        |\r\n",
      "\r\n",
      "\r\n",
      "|                                                                              |\r\n",
      "|                                                                              |\r\n",
      "|Certifications                                                                |\r\n",
      "\r\n",
      "\r\n",
      "|Microsoft: Programming in HTML5 with JavaScript and CSS3 (Examen 70-480)        |\r\n",
      "|Oracle Certified Associate Java Programmer OCA (En cour)                        |\r\n",
      "|                                                                                |\r\n",
      "|                                                                                |\r\n",
      "|                                                                                |\r\n",
      "|Centre d'intérêt                                                                |\r\n",
      "|                                                                                |\r\n",
      "|                                                                                |\r\n",
      "|JOGGING                                                                         |\r\n",
      "|Football                                                                        |\r\n",
      "|Lecture                                                                         |\r\n",
      "|                                                                                |\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\n",
      "['abdoulaye', 'barro', 'ingénieur', 'etude', 'développement', 'formation', '2018', 'master', 'big', 'data', 'aims', 'mbour', '2017', 'master', 'science', 'ingénieur', 'option', 'informatique', 'esp', '2015', 'master', 'statistique', 'appliquée', 'université', 'gaston', 'berger', 'mention', 'assez', 'bien', '2014', 'maitrise', 'mathématiques', 'appliquées', 'informatique', 'université', 'gaston', 'berger', 'mention', 'assez', 'bien', '2010', 'baccalauréat', 'scientifique', 'mention', 'passable', 'langues', 'anglais', 'intermédiaire', 'français', 'courant', 'localisation', 'dakar', 'keur', 'massar', 'principales', 'compétences', 'conception', 'analyse', 'développement', 'compétences', 'analyse', 'cahier', 'charges', 'fonctionnelles', 'modélisation', 'données', 'modèle', 'entité', 'association', 'modélisation', 'objet', 'uml', 'développement', 'application', 'algorithme', 'data', 'mining', 'compétences', 'techniques', 'domaine', 'niveau', 'domaine', 'niveau', 'langage', 'framework', 'bdd', 'sql', 'sql', '2', 'javascript', '2', 'server', 'jquery', 'postgresql', 'python', 'c', 'c', '1', '5', 'intégration', 'html', '2', 'css', 'java', 'java', 'jee', '2', 'php', 'bootstrap', '2', 'symfony', '1', '5', 'django', '1', 'systèmes', 'exploitation', 'linux', '2', 'windows', '2', '5', 'outils', 'maven', '1', 'uipath', '2', '5', 'tomcat', '2', '2', 'jboss', 'wildfly', '1', '5', 'rpa', 'express', 'zeplin', 'modélisation', 'uml', 'bpmn', '2', 'mathématiques', 'atos', 'déc', '2018', 'aujourd', 'hui', 'covoiturage', 'fonction', 'développeur', 'backend', 'projet', 'développement', 'application', 'covoiturage', 'mission', 'analyse', 'besoins', 'conception', 'réalisations', 'développement', 'application', 'déploiement', 'environnement', 'java', 'jee', 'jhipster', 'angular', 'html', 'css', 'maven', 'git', 'intelli', 'j', 'ionic', 'mysql', 'workbench', 'jdl', 'studio', 'atos', 'fev', '2017', 'déc', '2018', 'système', 'recommandation', 'fonction', 'data', 'scientist', 'projet', 'développement', 'système', 'recommandation', 'profil', 'mission', 'analyse', 'besoins', 'conception', 'réalisations', 'développement', 'partie', 'backend', 'python', 'développement', 'partie', 'front', 'angular', 'environnement', 'python', 'flask', 'angular', 'html', 'css', 'material', 'design', 'visual', 'studio', 'code', 'dimbl', 'holding', 'service', 'fev', '2017', 'juillet', 'développement', 'algorithme', '2017', 'fonction', 'stagiaire', 'data', 'scientist', 'projet', 'développement', 'algorithme', 'scraping', 'rebot', 'collecte', 'données', 'sites', 'web', 'mission', 'développer', 'algorithmes', 'collecte', 'données', 'réalisations', 'comparaison', 'programme', 'écrit', 'python', 'celui', 'écrit', 'c', 'environnement', 'r', 'python', 'c', 'certifications', 'microsoft', 'programming', 'in', 'html5', 'with', 'javascript', 'and', 'css3', 'examen', '70', '480', 'oracle', 'certified', 'associate', 'java', 'programmer', 'oca', 'en', 'cour', 'centre', 'intérêt', 'jogging', 'football', 'lecture']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter \n",
    "çà\n",
    "train,test =  dataset[0:11],dataset[11:]\n",
    "\n",
    "#2 tokenize : remove all no alphanumeric character and remove stopwords\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "print(train[0])\n",
    "\n",
    "def dataCleaning(raw_data):\n",
    "    cleaned_data = []\n",
    "    stop_words = set(stopwords.words(\"french\"))\n",
    "    for data in raw_data:\n",
    "        cleaned_data.append([x.lower() for x in tokenizer.tokenize(data) if x not in stop_words])\n",
    "    return cleaned_data;\n",
    "\n",
    "train = dataCleaning(train)\n",
    "\n",
    "print(train[0])\n",
    "occurences  = Counter(train[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wordDoc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-3406c844e7be>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtables\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtableToDF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwordDoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'wordDoc' is not defined"
     ]
    }
   ],
   "source": [
    "from docx import Document\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "#transform word table to pandas dataframe\n",
    "def tableToDF(wordDoc):\n",
    "    tables = []\n",
    "    for table in wordDoc.tables:\n",
    "        df = [['' for i in range(len(table.columns))] for j in range(len(table.rows))]\n",
    "        for i, row in enumerate(table.rows):\n",
    "            for j, cell in enumerate(row.cells):\n",
    "                if cell.text:\n",
    "                    df[i][j] = cell.text\n",
    "        tables.append(pd.DataFrame(df))\n",
    "    return tables;\n",
    "\n",
    "res = tableToDF(wordDoc)\n",
    "print(len(res))\n",
    "res[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = u\"./resume_sentence_dataset.json\"\n",
    "\n",
    "\n",
    "data = pd.read_csv(filepath)\n",
    "data.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
